{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e38106-6fe5-4f30-a220-7882b14e5cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing FetchLevelK Node ---\n",
      "Fetching HTML content from: https://perinim.github.io\n",
      "Extracted 12 links.\n",
      "Analyzing 12 links using LLM.\n",
      "LLM rejected link: https://perinim.github.io/\n",
      "LLM approved link: #\n",
      "LLM rejected link: https://perinim.github.io/projects/\n",
      "LLM rejected link: https://perinim.github.io/competitions/\n",
      "LLM approved link: https://perinim.github.io/cv/\n",
      "LLM rejected link: mailto:%70%65%72%69%6E%69%6D.%39%38@%67%6D%61%69%6C.%63%6F%6D\n",
      "LLM approved link: https://github.com/PeriniM\n",
      "LLM approved link: https://www.linkedin.com/in/perinim\n",
      "LLM rejected link: https://twitter.com/Perinim_98\n",
      "LLM approved link: https://jekyllrb.com/\n",
      "LLM approved link: https://github.com/alshedivat/al-folio\n",
      "LLM approved link: https://pages.github.com/\n",
      "Fetching HTML content from: https://perinim.github.io/cv/\n",
      "Extracted 25 links.\n",
      "Analyzing 25 links using LLM.\n",
      "LLM rejected link: https://perinim.github.io/\n",
      "LLM rejected link: https://perinim.github.io/\n",
      "LLM approved link: #\n",
      "LLM rejected link: https://perinim.github.io/projects/\n",
      "LLM rejected link: https://perinim.github.io/competitions/\n",
      "LLM rejected link: https://perinim.github.io/cv/\n",
      "LLM rejected link: #general-information\n",
      "LLM rejected link: #education\n",
      "LLM approved link: #work-experience\n",
      "LLM rejected link: #honors-and-awards\n",
      "LLM rejected link: #interests-hobbies\n",
      "LLM rejected link: https://www.bluetensor.ai/\n",
      "LLM rejected link: https://www.econox.nl/\n",
      "LLM rejected link: https://www.infominds.eu/\n",
      "LLM approved link: https://www.matrycs.eu/\n",
      "LLM rejected link: https://www.jeniot.it/\n",
      "LLM rejected link: https://www.soi.unitn.it/\n",
      "LLM rejected link: https://www.dana.com/\n",
      "LLM approved link: https://www.dana.com/\n",
      "LLM rejected link: https://www.dana.com/\n",
      "LLM approved link: https://www.unipd.it/en/\n",
      "LLM rejected link: https://github.com/PeriniM\n",
      "LLM approved link: https://jekyllrb.com/\n",
      "LLM approved link: https://github.com/alshedivat/al-folio\n",
      "LLM approved link: https://pages.github.com/\n",
      "--- Executing ParseNodeDepthK Node ---\n",
      "--- Executing DESCRIPTION Node ---\n",
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n",
      "--- Executing RAG Node ---\n",
      "--- Executing GANLK Node ---\n",
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'projects': 'NA'}\n"
     ]
    }
   ],
   "source": [
    "from scrapegraphai.graphs import DepthSearchGraph\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "open_ai_key = \"sk-YApzULCHGCaBlngoQKKjQqI9eMTjAlZsyBRIae3dMM8WvXXX\"\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": open_ai_key,\n",
    "        \"model\": \"openai/gpt-4o\",\n",
    "        \"base_url\": \"https://api.chatanywhere.tech/v1\"\n",
    "    },\n",
    "    \n",
    "    \"depth\": 2,  \n",
    "    \"only_inside_links\": True,\n",
    "    \"verbose\": True,\n",
    "    \"headless\": True   \n",
    "}\n",
    "search_graph = DepthSearchGraph(\n",
    "    prompt=\"List me all the projects with their description\",\n",
    "    source=\"https://perinim.github.io\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = search_graph.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee3af2-2aa4-45ba-ac01-58fc2064d10c",
   "metadata": {},
   "source": [
    "custom_graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95260ef0-9990-43a4-ab45-9f707db7e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing FetchLevelK Node ---\n",
      "Fetching HTML content from: https://perinim.github.io/\n",
      "Fetched content for https://perinim.github.io/ at depth 1\n",
      "Extracted 12 links.\n",
      "Analyzing 12 links using LLM.\n",
      "LLM approved link: https://perinim.github.io/\n",
      "LLM rejected link: #\n",
      "LLM approved link: https://perinim.github.io/projects/\n",
      "LLM approved link: https://perinim.github.io/competitions/\n",
      "LLM approved link: https://perinim.github.io/cv/\n",
      "LLM rejected link: mailto:%70%65%72%69%6E%69%6D.%39%38@%67%6D%61%69%6C.%63%6F%6D\n",
      "LLM rejected link: https://github.com/PeriniM\n",
      "LLM rejected link: https://www.linkedin.com/in/perinim\n",
      "LLM rejected link: https://twitter.com/Perinim_98\n",
      "LLM rejected link: https://jekyllrb.com/\n",
      "LLM rejected link: https://github.com/alshedivat/al-folio\n",
      "LLM rejected link: https://pages.github.com/\n",
      "Adding link to new documents: https://perinim.github.io/projects/\n",
      "Adding link to new documents: https://perinim.github.io/competitions/\n",
      "Adding link to new documents: https://perinim.github.io/cv/\n",
      "Fetching HTML content from: https://perinim.github.io/projects/\n",
      "Fetched content for https://perinim.github.io/projects/ at depth 2\n",
      "Extracted 13 links.\n",
      "Fetching HTML content from: https://perinim.github.io/competitions/\n",
      "Fetched content for https://perinim.github.io/competitions/ at depth 2\n",
      "Extracted 27 links.\n",
      "Fetching HTML content from: https://perinim.github.io/cv/\n",
      "Fetched content for https://perinim.github.io/cv/ at depth 2\n",
      "Extracted 25 links.\n",
      "--- Executing ParseNode Node ---\n",
      "--- Executing RAG Node ---\n",
      "Creating collection 'vectorial_collection'...\n",
      "Upserted 1 documents into the vector database.\n",
      "--- Executing GenerateAnswer Node ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'projects': 'NA'}\n"
     ]
    }
   ],
   "source": [
    "from scrapegraphai.graphs import CustomGraph\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "open_ai_key = \"sk-YApzULCHGCaBlngoQKKjQqI9eMTjAlZsyBRIae3dMM8WvXXX\"\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": open_ai_key,\n",
    "        \"model\": \"openai/gpt-4o\",\n",
    "        \"base_url\": \"https://api.chatanywhere.tech/v1\"\n",
    "    },\n",
    "    \n",
    "    \"depth\": 2,  \n",
    "    \"only_inside_links\": True,\n",
    "    \"verbose\": True,\n",
    "    \"headless\": True\n",
    "    \n",
    "    \n",
    "}\n",
    "graph = CustomGraph(\n",
    "    prompt=\"access the project page and list me all the projects with their description\",\n",
    "    source=\"https://perinim.github.io/\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = graph.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b869efd9-02a4-4b3f-9304-8ac1cd995fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing FetchLevelK Node ---\n",
      "Fetching HTML content from: https://perinim.github.io/projects\n",
      "Fetched content for https://perinim.github.io/projects at depth 1\n",
      "Extracted 13 links.\n",
      "Analyzing 13 links using LLM.\n",
      "LLM rejected link: https://perinim.github.io/\n",
      "LLM rejected link: https://perinim.github.io/\n",
      "LLM rejected link: #\n",
      "LLM approved link: https://perinim.github.io/projects/\n",
      "LLM rejected link: https://perinim.github.io/competitions/\n",
      "LLM rejected link: https://perinim.github.io/cv/\n",
      "LLM approved link: https://perinim.github.io/projects/rotary-pendulum-rl/\n",
      "LLM rejected link: https://github.com/PeriniM/DQN-SwingUp\n",
      "LLM rejected link: https://github.com/PeriniM/Multi-Agents-HAED\n",
      "LLM approved link: https://perinim.github.io/projects/wireless-esc-drone/\n",
      "LLM rejected link: https://jekyllrb.com/\n",
      "LLM rejected link: https://github.com/alshedivat/al-folio\n",
      "LLM rejected link: https://pages.github.com/\n",
      "--- Executing ParseNode Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- Executing GenerateAnswer Node ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'projects': [{'name': 'Rotary Pendulum RL', 'description': 'Open Source project aimed at controlling a real life rotary pendulum using RL algorithms'}, {'name': 'DQN Implementation from scratch', 'description': 'Developed a Deep Q-Network algorithm to train a simple and double pendulum'}, {'name': 'Multi Agents HAED', 'description': 'University project which focuses on simulating a multi-agent system to perform environment mapping. Agents, equipped with sensors, explore and record their surroundings, considering uncertainties in their readings.'}, {'name': 'Wireless ESC for Modular Drones', 'description': 'Modular drone architecture proposal and proof of concept. The project received maximum grade.'}]}\n"
     ]
    }
   ],
   "source": [
    "from scrapegraphai.graphs import CustomGraph\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "open_ai_key = \"sk-YApzULCHGCaBlngoQKKjQqI9eMTjAlZsyBRIae3dMM8WvXXX\"\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": open_ai_key,\n",
    "        \"model\": \"openai/gpt-4o\",\n",
    "        \"base_url\": \"https://api.chatanywhere.tech/v1\"\n",
    "    },\n",
    "    \n",
    "    \"depth\": 2,  \n",
    "    \"only_inside_links\": True,\n",
    "        \n",
    "    \"verbose\": True,\n",
    "    \"headless\": True\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "search_graph = CustomGraph(\n",
    "    prompt=\"List me all the projects with their description\",\n",
    "    source=\"https://perinim.github.io/\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = search_graph.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96704bf0-c3e1-428e-a1b5-d178c865d272",
   "metadata": {},
   "source": [
    "### for custom graph ，attempts 10 times, success 4 times， wrong anwer once， fail 5 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420d471-6d69-4c4f-9231-3858bf25a002",
   "metadata": {},
   "source": [
    "## baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7954258-a989-4aa9-8b1f-34c422a44d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To list all the projects with their descriptions from the given website, you need to navigate to the Projects section. Unfortunately, the HTML code you provided does not contain the detailed content of the Projects page. The list of projects is likely under the \"/projects/\" path on the website.\n",
      "\n",
      "Please follow these general steps:\n",
      "\n",
      "1. Go to the URL: [https://perinim.github.io/projects/](https://perinim.github.io/projects/).\n",
      "2. Analyze the HTML of that specific page to scrape the list of projects and their descriptions.\n",
      "\n",
      "If you have access to the HTML content of the '/projects/' page or need further assistance in scraping the data, please provide that content or let me know how I can further assist you.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "#Call the openai api directly\n",
    "# Initialize OpenAI client\n",
    "open_ai_key = \"sk-YApzULCHGCaBlngoQKKjQqI9eMTjAlZsyBRIae3dMM8WvXXX\"\n",
    "client = OpenAI(\n",
    "    api_key=open_ai_key,\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")\n",
    "\n",
    "def get_scraper_task(instruction):\n",
    "    \"\"\"\n",
    "    Generates a web scraping task based on the provided instruction.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The instruction containing the task details and URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The response content from the model or an error message if the API call fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call the OpenAI API with the provided instruction\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a web scraper. Follow the prompt in instruction to do task with the URL in the instruction.(note：You can use any way to get the html of a web page)\"},\n",
    "                {\"role\": \"user\", \"content\": instruction}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Return the content of the model's response\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "response = requests.get(\"https://perinim.github.io/\")\n",
    "response.raise_for_status()  # Ensure we raise an error for bad responses\n",
    "html_content = response.text\n",
    "import json\n",
    "instruction = {\n",
    "    \"prompt\": \"List me all the projects with their description\",\n",
    "    \"URL\": \"https://perinim.github.io\",\n",
    "    \"html\":html_content,\n",
    "}\n",
    "instruction_str = json.dumps(instruction)\n",
    "response = get_scraper_task(instruction_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddc45d-e37d-4304-95c3-9ccf9300acc4",
   "metadata": {},
   "source": [
    "### fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6193ef-2ef2-44b7-90cd-8992236f8de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
